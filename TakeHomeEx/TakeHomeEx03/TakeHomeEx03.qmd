---
title: "Take Home Exercise 3"
author: "Kieren Chua"
date: "November 1 2024"
date-modified: "last_modified"
execute: 
    eval: true # evaulate the code first
    echo: true # See the code output
    message: false # don't see the warnings
    freeze: true # Prevent re-render
---

# Part 1 : Data And Packages

## Packages

```{r}
pacman::p_load(sf, spdep, GWmodel, SpatialML, 
               tmap, rsample, Metrics, tidyverse,
               knitr, kableExtra, jsonlite)
```

## Data

We can get the lat long data from the previous output

```{r}
location_data <- read_rds("data/processed_data/coords.rds")

resale_data <- read_csv("data/resale.csv")

resale_tidy <- resale_data %>%
  mutate(address = paste(block,street_name)) %>%
  mutate(remaining_lease_yr = as.integer(
    str_sub(remaining_lease, 0, 2)))%>%
  mutate(remaining_lease_mth = as.integer(
    str_sub(remaining_lease, 9, 11)))
```

We also got data from the other required decision parameters, which are :

**Structural factors**

1.  Area of the unit
2.  Floor level Remaining
3.  lease
4.  Age of the unit

**Locational factors**

1.  Proxomity to CBD
2.  Proximity to eldercare
3.  Proximity to foodcourt/hawker centres
4.  Proximity to MRT
5.  Proximity to park
6.  Proximity to good primary school ( All schools are good schools lol)
7.  Proximity to shopping mall
8.  Proximity to supermarket
9.  Numbers of kindergartens within 350m
10. Numbers of childcare centres within 350m
11. Numbers of bus stop within 350m
12. Numbers of primary school within 1km

```{r}
CBD_lat_long <- c(1.287953, 103.851784) # Taken from https://www.latlong.net/place/downtown-core-singapore-20616.html

CBD_svy21 <- st_sfc(st_point(c(103.851784, 1.287953)), 
                    crs = 4326) %>%
                    st_transform(3414)
eldercare_data <- st_read(dsn = "data/EldercareServicesSHP", 
                          layer = "ELDERCARE") %>% st_transform(3414)
foodcourt_data <- st_read("data/HawkerCentresGEOJSON.geojson") %>% st_transform(3414)
MRT_data <- st_read("data/LTAMRTStationExitGEOJSON.geojson") %>% st_transform(3414)
park_data <- st_read("data/Parks.kml") %>% st_transform(3414)
primarySchool_data <- st_read("data/LTASchoolZone.geojson") %>% st_transform(3414)
mall_data <- st_read(dsn = "data/MP14SDCPPWPLANMallandPromenadeSHP", 
                     layer="G_MP14_PKWB_MALL_PROM_PL") %>% st_transform(3414)
supermarket_data <- st_read("data/SupermarketsGEOJSON.geojson") %>% st_transform(3414)
kindergarten_data <- st_read("data/Kindergartens.geojson") %>% st_transform(3414)
childcare_data <- st_read("data/ChildCareServices.geojson") %>% st_transform(3414)
busstop_data <- st_read(dsn = "data/BusStopLocation_Jul2024",
                        layer= "BusStop") %>% st_transform(3414)
```

# Part 2 : Processing the data

Now we need to add all the data to the dataframe. We will also jitter the geomoetries of dependent variables just in case they overlap.

## Locations of HDB

We can use the location data and join by postal code

```{r}
resale_tidy_loc <- left_join(resale_tidy, location_data, by = "address")
resale_tidy_clean <- resale_tidy_loc %>% 
                filter(!is.na(postal))

# Then we convert the lat long into SVY21
resale_sf <- st_as_sf(resale_tidy_clean, 
                      coords = c("longitude", "latitude"),
                      crs = 4326) %>%
            st_transform(3414)

# Handle the NA in the lease months
resale_sf$remaining_lease_mth[is.na(resale_sf$remaining_lease_mth)] <- 0
```

We also need to jitter the points so that the points do not share the same coordinates, we need to jitter quite abit for the regression to work latter.

```{r}
resale_sf$geometry <- st_jitter(resale_sf$geometry, amount = 2)
```

We can now show the data

```{r}
tmap_mode("plot")
tm_shape(resale_sf) + 
  tm_dots()
```

## Unit Age

```{r}
resale_sf$unit_age <- 99 - resale_sf$remaining_lease_yr
```

## Proximity to CBD

We can compare all locations to the single CBD coordinate and output a distance

```{r}
distance_matrix <- st_distance(resale_sf$geometry, CBD_svy21)

resale_sf$PROX_CBD <- apply(distance_matrix, 1, min)
```

## Proximity to Eldercare

In this case we can find the closest elder-care to the HDB unit

```{r}
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(eldercare_data$geometry, amount = 2))

resale_sf$PROX_ELDER <- apply(distance_matrix, 1, min)
```

## Proximity to Hawker Center

We do the same thing here

```{r}
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(st_zm(foodcourt_data$geometry), amount = 2))

resale_sf$PROX_HAWKER <- apply(distance_matrix, 1, min)
```

## Proximity to MRT

```{r}
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(st_zm(MRT_data$geometry), amount = 2))

resale_sf$PROX_MRT <- apply(distance_matrix, 1, min)
```

## Proximity to Park

```{r}
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(st_zm(park_data$geometry), amount = 2))

resale_sf$PROX_PARK <- apply(distance_matrix, 1, min)
```

## Proximity to Primary School

We need to get the center of the primary school to compare against centroid. Need to drop the z value

```{r}
primarySchool_data$geometry <- st_zm(primarySchool_data$geometry)
primarySchool_data$centroid <- st_centroid(primarySchool_data$geometry)
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(primarySchool_data$centroid, amount = 2))

resale_sf$PROX_PRIM <- apply(distance_matrix, 1, min)
```

## Proximity to Shopping Mall

Same for the shopping mall

```{r}
mall_data$centroid <- st_centroid(mall_data$geometry)
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(mall_data$centroid, amount = 2))

resale_sf$PROX_MALL <- apply(distance_matrix, 1, min)
```

## Proximity to Supermarket

```{r}
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(st_zm(supermarket_data$geometry), amount = 2))

resale_sf$PROX_SPMK <- apply(distance_matrix, 1, min)
```

## Number of Kindergartens within 350m

To calculate the number of kindergartens within 350m of the HBD, we need to have a 350m search radius around each location, then count the number of kindergartens within

```{r}
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(st_zm(kindergarten_data$geometry), amount = 2))

count_within_350m <- apply(distance_matrix, 1, function(distances) {
  sum(distances <= 350)  # Count points within 350 meters
})

resale_sf$KIND_350 <- count_within_350m

```

## Number of Childcares within 350m

```{r}
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(st_zm(childcare_data$geometry), amount = 2))

count_within_350m <- apply(distance_matrix, 1, function(distances) {
  sum(distances <= 350)  # Count points within 350 meters
})

resale_sf$CHILD_350 <- count_within_350m
```

## Number of Bus-Stops within 350m

```{r}
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(busstop_data$geometry, amount = 2))

count_within_350m <- apply(distance_matrix, 1, function(distances) {
  sum(distances <= 350)  # Count points within 350 meters
})

resale_sf$BUS_350 <- count_within_350m
```

## Number of Primary School within 1000m

```{r}
distance_matrix <- st_distance(resale_sf$geometry, st_jitter(primarySchool_data$centroid, amount = 2))

count_within_1km <- apply(distance_matrix, 1, function(distances) {
  sum(distances <= 1000)  # Count points within 350 meters
})

resale_sf$PRI_1K <- count_within_1km
```

## Saving the data

Now we can save the data for future purposes.

```{r}
write_rds(resale_sf, "data/resale_sf_processed.rds")
```

# Part 3 : Shrinking the search space

## Read the data

```{r}
cleaned_resale_sf <- read_rds("data/resale_sf_processed.rds")
cleaned_resale_no_geom <- cleaned_resale_sf %>% st_drop_geometry()
```

Because there is too much data, we will need to reduce the size of inspection. First we shall determine the types of flats available.

```{r}
unique_flat_types <- unique(cleaned_resale_sf$flat_type)
unique_flat_types
```

We also want to see the types of flats that are available

```{r}
unique_flat_models <- unique(cleaned_resale_sf$flat_model)
unique_flat_models
```

To have more focus on the data, we shall focus on the more expensive flat models vs the exercise given to us. To get an idea of what is expensive, we will need to see the spread of flat prices by their specific prices. We can get the mean price of each house type 

```{r}
mean_prices <- cleaned_resale_sf %>%
  group_by(flat_model) %>%
  summarise(mean_price = mean(resale_price, na.rm = TRUE))
print(mean_prices)
```

We can also see the mean prices by flat type

```{r}
mean_prices <- cleaned_resale_sf %>%
  group_by(flat_type) %>%
  summarise(mean_price = mean(resale_price, na.rm = TRUE))
print(mean_prices)
```


Since the class exercise did the model for 2 and 3 room flat, we shall try to do a housing market that is higher in value which is the executive flats.

```{r}
cleaned_resale_sf_cut <- cleaned_resale_sf %>% filter(flat_type %in% c('EXECUTIVE'))

cleaned_resale_sf_cut_no_geom <- cleaned_resale_sf_cut %>%
                                  st_drop_geometry()
```

This leaves us with over 1000 data samples, which should be significant enough for us.

```{r}
print(nrow(cleaned_resale_sf_cut))
```

# Part 4 : Computing Correlation Matrix

## Bin the data

We need to bin some of the variables so that they make integers

### Storeys

```{r}
unqiue_storey <- unique(cleaned_resale_sf_cut_no_geom$storey_range)
unqiue_storey
```

We need to map it by height, where a low value corresponds to a low height

```{r}
# We need to arrange the the mapping by height
calculate_height <- function(range) {
  # Split the range to get the lower and upper bounds
  bounds <- as.numeric(unlist(strsplit(range, " TO ")))

  avg_height <- mean(bounds)
  return(avg_height)
}

heights <- sapply(unqiue_storey, calculate_height)

# Make the mapping
height_mapping <- setNames(sapply(unqiue_storey, calculate_height), unqiue_storey)
```

We then apply this mapping to the dataframe 

```{r}
cleaned_resale_sf_cut_no_geom$storey_range_bin <- height_mapping[cleaned_resale_sf_cut_no_geom$storey_range]
cleaned_resale_sf_cut$storey_range_bin <- height_mapping[cleaned_resale_sf_cut$storey_range]
```

## Plotting the graph

We are not sure if all the variables are correlated, so we can build a correlation matrix to see if we need to exclude any variables

```{r}
required_cols <- c(7, 9, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29)
corrplot::corrplot(cor(cleaned_resale_sf_cut_no_geom[, required_cols]), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.5, 
                   method = "number", 
                   type = "upper")
```

We can see that lease commence date and remain lease yr have high correlation, so we remove lease commence date.

## Variance Inflation Factor

We also can check the Variance Inflation Factor to see if there are any variables above a 5

### Train-Test Split

First we need to do a train test split for any model training. Split shall be 65 - 35.

```{r}

set.seed(1234)
# First we try to remove any NA values
cleaned_resale_sf_cut <- cleaned_resale_sf_cut[rowSums(is.na(st_drop_geometry(cleaned_resale_sf_cut))) == 0,, ]

resale_split <- initial_split(cleaned_resale_sf_cut, 
                              prop = 6.5/10,)

train_data <- training(resale_split)

```

We need to check for overlaps in the train data to see if there is data we need to remove.

```{r}
overlap_matrix <- st_equals(train_data$geometry)
overlap_matrix <- sapply(overlap_matrix, function(x) length(x) > 1)
any_overlap <- any(overlap_matrix)
any_overlap
```

### Generating simple LM Model

```{r}
price_mlr <- lm(resale_price ~ floor_area_sqm + storey_range_bin + remaining_lease_yr +
                  PROX_CBD + PROX_ELDER + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL +
                  PROX_SPMK + KIND_350 +
                  CHILD_350 + BUS_350 +
                  PRI_1K,
                data=train_data)
```

```{r}
vif <- performance::check_collinearity(price_mlr)
kable(vif, 
      caption = "Variance Inflation Factor (VIF) Results") %>%
  kable_styling(font_size = 18) 
```

We can also plot this out for better visualization

```{r}
plot(vif) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Part 5 : Generating Geographically Weighted Predictive Models

## Convert to Spatial Datframe

```{r}
# First we check for NA values in the traindata

train_data_sp <- as_Spatial(train_data)
```

## Get adaptive bandwidth

```{r}
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  storey_range_bin + remaining_lease_yr +
                  PROX_CBD + PROX_ELDER + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SPMK + KIND_350 +
                  CHILD_350 + BUS_350 +
                  PRI_1K,
                  data=train_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

We will also save it for the future

```{r}
write_rds(bw_adaptive, "data/model/bw_adaptive.rds")
```

```{r}
bw_adaptive <- read_rds("data/model/bw_adaptive.rds")
```

## Make the model

Now we will make the adaptive GWR model

```{r}
gwr_adaptive <- gwr.basic(formula = resale_price ~ floor_area_sqm +
                  storey_range_bin + remaining_lease_yr +
                  PROX_CBD + PROX_ELDER + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SPMK + KIND_350 +
                  CHILD_350 + BUS_350 +
                  PRI_1K,
                          data=train_data_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)

```

Save a copy

```{r}
write_rds(gwr_adaptive, "data/model/gwr_adaptive.rds")
```

## Reading the model

```{r}
gwr_adaptive <- read_rds("data/model/gwr_adaptive.rds")
```

```{r}
gwr_adaptive
```

From the results we can see that proximity to eldercare, hawker centers, park, mall and the numbers of  kindergartens and childcares within 350m and primary schools within 1km all have a p-value more than 5%.

**Note** : This is expected since these are higher value properties that are usually owned by affluent individuals who probably have no dependents, have barely any free time to relax and probably have their own mode of transport, hence the lack of statistical significance for aforementioned variables with exception to MRT stations which has been linked to property value. Cannot acertain this statistic since both income and family data for individuals living in these apartments is not available to us.

## Computer Test Data Adaptive Bandwidth

```{r}
test_data <- testing(resale_split)
```

We also need to check for overlap in the test data
```{r}
overlap_matrix <- st_equals(test_data$geometry)
overlap_matrix <- sapply(overlap_matrix, function(x) length(x) > 1)
any_overlap <- any(overlap_matrix)
any_overlap
```

```{r}
test_data_sp <- as_Spatial(test_data)
```

```{r}
gwr_bw_test_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  storey_range_bin + remaining_lease_yr +
                  PROX_CBD + PROX_ELDER + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SPMK + KIND_350 +
                  CHILD_350 + BUS_350 +
                  PRI_1K,
                  data=test_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

Now we can run prediction on the test dataset

## Running Predictions on the test data

```{r}
st_crs(train_data_sp)
```

```{r}
gwr_pred <- gwr.predict(formula = resale_price ~ floor_area_sqm +
                  storey_range_bin + remaining_lease_yr +
                  PROX_CBD + PROX_ELDER + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SPMK + KIND_350 +
                  CHILD_350 + BUS_350 +
                  PRI_1K, 
                  data= train_data_sp, 
                  predictdata = test_data_sp, 
                  bw=bw_adaptive, 
                  kernel = 'gaussian', 
                  adaptive= TRUE, 
                  longlat = FALSE)
```

## Finding the Errors and Plotting Residuals 

### RMSE

Now we want to see how effective is our model by computing the residuals

```{r}
Gwr_pred_df <- as.data.frame(gwr_pred$SDF)

test_data_gwr_pred <- cbind(test_data,
                        Gwr_pred_df)
```

First we can compare the RMSE.

```{r}
rmse(test_data_gwr_pred$resale_price, 
     test_data_gwr_pred$prediction)
```

```{r}
ggplot(data = test_data_gwr_pred,
       aes(x = prediction,
           y = resale_price)) +
  geom_point()
```
### Plot Residuals

We can also plot this out on the map to see the residuals by location

```{r}
test_data_gwr_pred$residuals <- test_data_gwr_pred$prediction - test_data_gwr_pred$resale_price
st_crs(test_data_gwr_pred)
```

```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL") %>% st_transform(3414)
```

```{r}
tmap_mode("view")
tm_shape(mpsz) +
    tmap_options(check.and.fix = TRUE) +
    tm_polygons(alpha = 0.4) +
tm_shape(test_data_gwr_pred) +
    tm_dots(col = "residuals",
            alpha = 0.6,
            style = "quantile")
tmap_mode("plot")
```



# Part 7 : Generating Random Forest Model

Now we can move on to creating the random forest model

```{r}
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)
train_data_nogeom <- train_data %>%
  st_drop_geometry()
```

After preparing the data, we can train the model below

## Basic Random Forest

```{r}
set.seed(1234)
rf <- ranger(resale_price ~ floor_area_sqm +
                  storey_range_bin + remaining_lease_yr +
                  PROX_CBD + PROX_ELDER + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SPMK + KIND_350 +
                  CHILD_350 + BUS_350 +
                  PRI_1K,
             data=train_data_nogeom)
```

We can view the model output below

```{r}
rf
```

## Cleaning up the data

The code now is taking up alot of space, so we nedd to clean up some of the data that we dont need for future

```{r}
rm(rf)
rm(price_mlr)
rm(resale_split)
rm(gwr_adaptive)
rm(busstop_data)
rm(childcare_data)
rm(cleaned_resale_no_geom)
rm(cleaned_resale_sf)
rm(eldercare_data)
rm(foodcourt_data)
rm(kindergarten_data)
rm(mall_data)
rm(MRT_data)
rm(primarySchool_data)
```

## Geographically weighted Random Forest

```{r}
set.seed(1234)
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm +
                  storey_range_bin + remaining_lease_yr +
                  PROX_CBD + PROX_ELDER + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SPMK + KIND_350 +
                  CHILD_350 + BUS_350 +
                  PRI_1K,
                     dframe=train_data_nogeom, 
                     bw=bw_adaptive,
                     kernel="adaptive",
                     coords=coords_train)
```

We can then save the model for future use

```{r}
write_rds(gwRF_adaptive, "data/model/gwRF_adaptive.rds")
```

We then re-read it, mostly for running purposes

```{r}
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

## Predicting with test data

```{r}
test_data_nogeom <- cbind(
  test_data, coords_test) %>%
  st_drop_geometry()
```

```{r}
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data_nogeom, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```


```{r}
GRF_pred_df <- as.data.frame(gwRF_pred)

test_data_pred <- cbind(test_data,
                        GRF_pred_df)
```

Save the data for the future

```{r}
write_rds(test_data_pred, "data/test_results.rds")
```

```{r}
test_data_pred <- read_rds( "data/test_results.rds")
```

## Freeing Memory

The model is very large >15Gb so once we got th results, we should free up the memory

```{r}
rm(gwRF_adaptive)
```

## Viewing Random Forest Prediction Error

Now we can compare the difference in values from predictions vs actual resale value by location

```{r}
rmse(test_data_pred$resale_price, 
     test_data_pred$gwRF_pred)
```

```{r}
ggplot(data = test_data_pred,
       aes(x = gwRF_pred,
           y = resale_price)) +
  geom_point()
```


## Show residuals

```{r}
test_data_pred$residuals <- test_data_pred$gwRF_pred - test_data_pred$resale_price
st_crs(test_data_pred)
```

```{r}
# Load in the mpsz data
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL") %>% st_transform(3414)
```

Plot the map

```{r}
tmap_mode("view")
tm_shape(mpsz) +
    tmap_options(check.and.fix = TRUE) +
    tm_polygons(alpha = 0.4) +
tm_shape(test_data_pred) +
    tm_dots(col = "residuals",
            alpha = 0.6,
            style = "quantile")
tmap_mode("plot")
```
